# 大模型 Pre-Training.md

## LayerNorm vs BatchNorm

Transformer 中使用 LayerNorm 而不是 BatchNorm

Norm => Normalization 归一化

归一化是将数据缩放到0-1之间，标准化是将数据缩放到均值为0，方差为1的正态分布

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E6%90%9E%E6%B8%85%E6%A5%9A%E4%B8%BA%E4%BB%80%E4%B9%88Transformer%E4%B8%AD%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm/0.png)

## BatchNorm

BatchNorm 旨在提高神经网络和训练速度、稳定和性能

BatchNorm 主要解决的问题是在训练深度神经网络时出现的内部协变量偏移（Internal Covariate Shift），即网络中各层输入数据分布的变化

内部协变量偏移是指，随着网络层次的加深，每一层的输入数据（即前一层的输出）的分布可能会发生变化

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E6%90%9E%E6%B8%85%E6%A5%9A%E4%B8%BA%E4%BB%80%E4%B9%88Transformer%E4%B8%AD%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm/1.png)

## LayerNorm

Layer Normalization（层归一化）是一种在深度学习中用于稳定神经网络训练和提高性能的技术

可以减少 BatchNorm 中的噪声问题，并允许网络使用更小的小批量大小进行训练

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E6%90%9E%E6%B8%85%E6%A5%9A%E4%B8%BA%E4%BB%80%E4%B9%88Transformer%E4%B8%AD%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm/2.png)

## Why LayerNorm

我们需要确保参与归一化的数据点在本质上是可比的

LayerNorm的解决方案是对每个样本的所有特征进行单独归一化，而不是基于整个批次。这就像是评估每个学生在所有科目中的表现，而不是仅仅关注单一科目，这样可以更全面地理解每个学生的整体表现

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E6%90%9E%E6%B8%85%E6%A5%9A%E4%B8%BA%E4%BB%80%E4%B9%88Transformer%E4%B8%AD%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm/3.png)

# 混合专家模型 (MoE）

## 混合专家模型 (MoEs):

- 与稠密模型相比， 预训练速度更快
- 与具有相同参数数量的模型相比，具有更快的 推理速度
- 需要 大量显存，因为所有专家系统都需要加载到内存中
- 在微调方面存在诸多挑战

## 什么是 MoE

模型规模是提升模型性能的关键因素之一。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B%20%28MoE%29%20%E8%AF%A6%E8%A7%A3%EF%BC%88%E8%8A%82%E9%80%89%EF%BC%89/0.png)

## 什么是稀疏性

条件计算的概念 (即仅在每个样本的基础上激活网络的不同部分) 使得在不增加额外计算负担的情况下扩展模型规模成为可能。这一策略在每个 MoE 层中实现了数以千计甚至更多的专家的有效利用

# Mamba

OpenAI 的 ChatGPT、Google 的 Gemini 和 GitHub 的 Copilot 都是由 Transformer 驱动的.然而，Transformer 有一个根本缺陷：它是由 Attention 驱动，而 Attention 会随着序列长度的平方增长

对于快速交流（要求 ChatGPT 讲个笑话），这没问题.但对于需要大量单词的查询（要求 ChatGPT 总结一份 100 页的文档），Transformer 可能会变得非常慢

## 背景

Mamba 架构主要基于 S4。这是一种最新的状态空间模型 state space model (SSM) 架构

SSM 旨在很好地处理音频、传感器数据和图像等连续数据

# 多模态大模型 Multimodality

多模态（Multimodality）是指集成和处理两种或两种以上不同类型的信息或数据的方法和技术

多模态涉及的数据类型通常包括但不限于文本、图像、视频、音频和传感器数据

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%88Multimodal%20LLMs%EF%BC%89/00.png)

MLLMs 的核心优势在于它们能够处理和理解来自不同模态的信息，并将这些信息融合以完成复杂的任务

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%88Multimodal%20LLMs%EF%BC%89/0.png)

组成部分包括

- Modality Encoder：负责将不同模态的输入数据编码为模型可理解的表示；
- Input Projector：将不同模态的输入数据映射到共享的语义空间；
- LLMs：大型语言模型，用于处理文本数据；
- Output Projector：将模型生成的输出映射回原始模态的空间；
- Modality Generator：根据输入数据生成对应的输出数据

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E4%BA%86%E8%A7%A3%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%EF%BC%88Multimodal%20LLMs%EF%BC%89/2.png)
