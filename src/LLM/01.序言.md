# 序言 AGI 之路

## Scaling Law

“规模定律”：在一定条件下，系统的性能或特性，随着规模的变化而呈现出规律性的变化

要素：
1. 模型大小（参数数量）
1. 数据集大小
1. 计算资源

![alt text](https://luhengshiwo.github.io/LLMForEverybody/00-%E5%BA%8F-AGI%E4%B9%8B%E8%B7%AF/assest/%E5%A4%A7%E5%AE%B6%E9%83%BD%E8%B0%88%E7%9A%84ScalingLaw%E6%98%AF%E4%BB%80%E4%B9%88/0.png)

为什么要研究 Scaling Law：
1. 预测模型性能
1. 资源优化

## 涌现

涌现：在复杂系统中，又大量、简单个体的相互作用产生的不可预测、新的模型或行为

Transformer 起源

## Perplexity

Perplexity：度量语言模型好坏的一种指标。即做预测时需要从多少个候选字中选出正确答案

## Pre-Training 预训练

大模型训练通用过程：前向传播（forward pass）和反向传播（backward pass）

传播单位为 unit

![text](https://mmbiz.qpic.cn/sz_mmbiz_png/RMYib02oIiaWUJu0r4NXNh0ysWxiahseRcMOccMb51PMHjoRibgaUYYIN3rxO1UQ6PrrQX95dDvBPxib2lNAQDp23Zg/640?wx_fmt%3Dpng%26from%3Dappmsg%26tp%3Dwebp%26wxfrom%3D5%26wx_lazy%3D1%26wx_co%3D1)

## MOE vs Dense

- 专家混合模型（Mixture-of-Experts，MoE）

- Llama3.1 采用的是稠密（Dense）模型
