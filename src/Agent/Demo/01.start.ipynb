{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5ba7f21",
   "metadata": {},
   "source": [
    "## å®‰è£…ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088febf4",
   "metadata": {},
   "source": [
    "\n",
    "1. å®‰è£… pyenv\n",
    "\n",
    "```bash\n",
    "brew install pyenv\n",
    "\n",
    "pyenv install 3.12.10\n",
    "\n",
    "pyenv global 3.12.10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0266dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaec1d74",
   "metadata": {},
   "source": [
    "2. å®‰è£… Langchain åŒ…\n",
    "\n",
    "```bash\n",
    "pip install langchain\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae4591",
   "metadata": {},
   "source": [
    "3. å®‰è£… openai\n",
    "\n",
    "```bash\n",
    "pip install openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c3edae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.23\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /Users/liam.liu/.pyenv/versions/3.12.10/lib/python3.12/site-packages\n",
      "Requires: langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: \n",
      "Name: openai\n",
      "Version: 0.28.1\n",
      "Summary: Python client library for the OpenAI API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: OpenAI\n",
      "Author-email: support@openai.com\n",
      "License: \n",
      "Location: /Users/liam.liu/.pyenv/versions/3.12.10/lib/python3.12/site-packages\n",
      "Requires: aiohttp, requests, tqdm\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "! pip show langchain\n",
    "\n",
    "! pip show openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af90d2c",
   "metadata": {},
   "source": [
    "## è¿è¡Œ openai\n",
    "\n",
    "1. å®šä¹‰ç¯å¢ƒå˜é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "089d2dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai_api_key: sk-xxx ; openai_api_key: https://api.deepseek.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "\n",
    "print(\"openai_api_key:\", openai_api_key, \"; openai_api_key:\", openai_api_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8df02",
   "metadata": {},
   "source": [
    "2. è¿è¡Œ openai å®˜æ–¹ SDKï¼ˆDeepseek ä¸¾ä¾‹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "548367b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key, base_url=openai_api_base)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
    "    ],\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "# will print: Hello! How can I assist you today?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962287ae",
   "metadata": {},
   "source": [
    "3. ä½¿ç”¨ Langchain è¿è¡Œ LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2812e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚"
     ]
    }
   ],
   "source": [
    "# from langchain.llms import OpenAI\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "# llm = OpenAI(\n",
    "#     model=\"gpt-4o\",\n",
    "#     openai_api_key=api_key,\n",
    "#     openai_api_base=api_base\n",
    "#     temperature=0,\n",
    "#     )\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=openai_api_key,\n",
    "    api_base=openai_api_base,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    # max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\"system\", \"You are a helpful translator. Translate the user sentence to Chinese.\"),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_message = llm.invoke(messages)\n",
    "# AIMessage(content='æˆ‘å–œæ¬¢ç¼–ç¨‹ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3, 'prompt_tokens': 20, 'total_tokens': 23, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 20}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3d5141a69a_prod0225', 'id': '854c5b87-2b07-49d3-a098-34c463dffbcf', 'finish_reason': 'stop', 'logprobs': None}, id='run-f8e98f70-afb1-43cb-bdb7-b57701b3e757-0', usage_metadata={'input_tokens': 20, 'output_tokens': 3, 'total_tokens': 23, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})\n",
    "\n",
    "# content\n",
    "print(ai_message.content)\n",
    "\n",
    "# stream \n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.text(), end=\"\")\n",
    "\n",
    "# async\n",
    "# await llm.ainvoke(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd924274",
   "metadata": {},
   "source": [
    "4. èµ·åå¤§å¸ˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75567c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ,è¯·æ¨¡ä»¿ç¤ºä¾‹èµ·3ä¸ªä¸­å›½ç‰¹è‰²çš„åå­—,æ¯”å¦‚ç”·å­©ç»å¸¸è¢«å«åšç‹—è›‹,å¥³å­©ç»å¸¸è¢«å«åšç¿ èŠ±\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/td/dz_bhv6d2dd01px1035pzm2m0000gp/T/ipykernel_74663/685618045.py:17: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm.predict(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å¥½çš„ï¼æ ¹æ®ä¸­å›½æ°‘é—´ä¼ ç»Ÿï¼Œå¾ˆå¤šæ¥åœ°æ°”çš„åå­—ä¼šä½¿ç”¨æœ´å®çš„å­—çœ¼æˆ–\"è´±åå¥½å…»æ´»\"çš„ä¹ ä¿—ï¼Œä»¥ä¸‹æ˜¯3ç»„ç‰¹è‰²åå­—ç¤ºä¾‹ï¼Œå¹¶é™„ä¸Šé£æ ¼è¯´æ˜ï¼š\\n\\n**ç”·å­©å**  \\n1. **é“æŸ±** - é’¢é“èˆ¬ç»“å®ï¼Œä½“ç°å¯¹å¥åº·ä½“é­„çš„æœŸç›¼  \\n2. **äºŒç‰›** - æ’è¡Œ+ç‰²ç•œåï¼Œå¯“æ„åƒç‰›ä¸€æ ·å‹¤åŠ³èƒ½å¹²  \\n3. **æ “å­** - æ—§æ—¶è®¤ä¸º\"æ‹´ä½\"å­©å­èƒ½é¿å…å¤­æŠ˜ï¼Œå¸¦ç¥ˆç¦æ„å‘³  \\n\\n**å¥³å­©å**  \\n1. **æ‹›å¨£** - åæ˜ ä¼ ç»Ÿå®¶åº­å¯¹ç”·å­©çš„æœŸå¾…ï¼ˆè°éŸ³\"æ‹›å¼Ÿ\"ï¼‰  \\n2. **å°èŠ³** - å…­ä¸ƒåå¹´ä»£ç»å…¸æ‘å§‘åï¼Œå……æ»¡ä¹¡åœŸæ°”æ¯  \\n3. **èƒ–ä¸«** - ç”¨ä½“å‹ç‰¹å¾å–åï¼Œä½“ç°\"èƒ–=æœ‰ç¦æ°”\"çš„è§‚å¿µ  \\n\\n**ç‰¹æ®Šè¯´æ˜**ï¼šè¿™ç±»åå­—å¸¸è§äº20ä¸–çºªä¸­å‰æœŸï¼Œå¦‚ä»Šæ›´å¤šä½œä¸ºäº²åˆ‡çš„æ˜µç§°æˆ–ç½‘ç»œè°ƒä¾ƒä½¿ç”¨ï¼Œå®é™…å–åå·²è¶‹å‘æ–‡é›…åŒ–ã€‚éœ€è¦å…¶ä»–é£æ ¼çš„åå­—å¯ä»¥å‘Šè¯‰æˆ‘å“¦ï¼'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=openai_api_key,\n",
    "    api_base=openai_api_base,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ,è¯·æ¨¡ä»¿ç¤ºä¾‹èµ·3ä¸ª{county}çš„åå­—,æ¯”å¦‚ç”·å­©ç»å¸¸è¢«å«åš{boy},å¥³å­©ç»å¸¸è¢«å«åš{girl}\")\n",
    "message = prompt.format(county=\"ä¸­å›½ç‰¹è‰²\",boy=\"ç‹—è›‹\",girl=\"ç¿ èŠ±\")\n",
    "print(message)\n",
    "\n",
    "llm.invoke(message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b815c3ef",
   "metadata": {},
   "source": [
    "5. æ ¼å¼åŒ–è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddde2303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'bye']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "# è‡ªå®šä¹‰classï¼Œç»§æ‰¿äº† BaseOutputParser\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser):\n",
    "    \"\"\"Parse the output of an LLM call to a comma-separated list.\"\"\"\n",
    "\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "\n",
    "CommaSeparatedListOutputParser().parse(\"hi, bye\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5039d87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ,è¯·æ¨¡ä»¿ç¤ºä¾‹èµ·3ä¸ªå…·æœ‰ç¾å›½ç”·å­©ç‰¹è‰²çš„åå­—,ç¤ºä¾‹ï¼šç”·å­©å¸¸ç”¨åsam,å¥³å­©å¸¸ç”¨ålucyã€‚è¯·è¿”å›ä»¥é€—å·åˆ†éš”çš„åˆ—è¡¨å½¢å¼ã€‚ä»…è¿”å›é€—å·åˆ†éš”çš„åˆ—è¡¨ï¼Œä¸è¦è¿”å›å…¶ä»–å†…å®¹ã€‚\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jack,ethan,noah']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatDeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=openai_api_key,\n",
    "    api_base=openai_api_base,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    ")\n",
    "prompt = PromptTemplate.from_template(\"ä½ æ˜¯ä¸€ä¸ªèµ·åå¤§å¸ˆ,è¯·æ¨¡ä»¿ç¤ºä¾‹èµ·3ä¸ªå…·æœ‰{county}ç‰¹è‰²çš„åå­—,ç¤ºä¾‹ï¼šç”·å­©å¸¸ç”¨å{boy},å¥³å­©å¸¸ç”¨å{girl}ã€‚è¯·è¿”å›ä»¥é€—å·åˆ†éš”çš„åˆ—è¡¨å½¢å¼ã€‚ä»…è¿”å›é€—å·åˆ†éš”çš„åˆ—è¡¨ï¼Œä¸è¦è¿”å›å…¶ä»–å†…å®¹ã€‚\")\n",
    "\n",
    "message = prompt.format(county=\"ç¾å›½ç”·å­©\",boy=\"sam\",girl=\"lucy\")\n",
    "print(message)\n",
    "\n",
    "ai_message = llm.invoke(message)\n",
    "CommaSeparatedListOutputParser().parse(ai_message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
