{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec387d4d",
   "metadata": {},
   "source": [
    "# LLM 调用\n",
    "\n",
    "1. LLM 调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd2fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.messages import HumanMessage,AIMessage\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://api.deepseek.com\"\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    temperature=0,\n",
    "    openai_api_key = openai_api_key,\n",
    "    openai_api_base = openai_api_key\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    AIMessage(role=\"system\",content=\"你好，我是tomie！\"),\n",
    "    HumanMessage(role=\"user\",content=\"你好tomie，我是狗剩!\"),\n",
    "    AIMessage(role=\"system\",content=\"认识你很高兴!\"),\n",
    "    HumanMessage(role=\"user\",content=\"你知道我叫什么吗？\")\n",
    "]\n",
    "\n",
    "response = chat.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aa807f",
   "metadata": {},
   "source": [
    "2. 流式调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68358b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM类大模型的流式输出方法\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "api_base = os.getenv(\"OPENAI_PROXY\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#构造一个llm\n",
    "llm = OpenAI(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    openai_api_key = api_key,\n",
    "    openai_api_base = api_base,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "for chunk in llm.stream(\"写一首关于秋天的诗歌\"):\n",
    "    print(chunk,end=\"\",flush=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888492f",
   "metadata": {},
   "source": [
    "3. Token 追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM的toekn追踪\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import os\n",
    "api_base = os.getenv(\"OPENAI_PROXY\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#构造一个llm\n",
    "llm = OpenAI(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    openai_api_key = api_key,\n",
    "    openai_api_base = api_base,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "# 通过 callback 来驱动\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"给我讲一个笑话\")\n",
    "    print(result)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatmodels的token追踪\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import os\n",
    "api_base = os.getenv(\"OPENAI_PROXY\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model = \"gpt-4\",\n",
    "    temperature=0,\n",
    "    openai_api_key = api_key,\n",
    "    openai_api_base = api_base,\n",
    "    max_tokens=512,\n",
    ")\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm.invoke(\"给我讲一个笑话\")\n",
    "    print(result)\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d70f9",
   "metadata": {},
   "source": [
    "4. 自定义输出\n",
    "\n",
    "- 输出函数参数\n",
    "- 输出 json\n",
    "- 输出 list\n",
    "- 输出日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#讲笑话机器人：希望每次根据指令，可以输出一个这样的笑话(小明是怎么死的？笨死的)\n",
    "\n",
    "from langchain.llms import  OpenAI\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.pydantic_v1 import BaseModel,Field,validator\n",
    "from typing import  List\n",
    "import os\n",
    "api_base = os.getenv(\"OPENAI_PROXY\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# 构造LLM\n",
    "model = OpenAI(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    openai_api_key = api_key,\n",
    "    openai_api_base = api_base,\n",
    ")\n",
    "\n",
    "# 定义个数据模型，用来描述最终的实例结构\n",
    "class Joke(BaseModel):\n",
    "    setup:str = Field(description=\"设置笑话的问题\")\n",
    "    punchline:str = Field(description=\"回答笑话的答案\")\n",
    "\n",
    "    #验证问题是否符合要求\n",
    "    @validator(\"setup\")\n",
    "    def question_mark(cls,field):\n",
    "        if field[-1] != \"？\":\n",
    "            raise ValueError(\"不符合预期的问题格式!\")\n",
    "        return field\n",
    "\n",
    "# 将Joke数据模型传入\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"回答用户的输入.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables = [\"query\"],\n",
    "    partial_variables = {\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "prompt_and_model = prompt | model # 管道操作符\n",
    "out_put = prompt_and_model.invoke({\"query\":\"给我讲一个笑话\"})\n",
    "\n",
    "print(\"out_put:\",out_put)\n",
    "# out_put: {\"setup\": \"为什么猫咪总是喜欢把东西丢到地上？\", \"punchline\": \"因为它们觉得地球是圆的，所以才会有东西掉下来。\"}\n",
    "\n",
    "parser.invoke(out_put)\n",
    "# Joke(setup='为什么猫咪总是喜欢把东西丢到地上？', punchline='因为它们觉得地球是圆的，所以才会有东西掉下来。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e5a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM的输出格式化成python list形式，类似['a','b','c']\n",
    "\n",
    "from langchain.output_parsers import  CommaSeparatedListOutputParser\n",
    "from langchain.prompts import  PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "api_base = os.getenv(\"OPENAI_PROXY\")\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "#构造LLM\n",
    "model = OpenAI(\n",
    "    model = \"gpt-3.5-turbo-instruct\",\n",
    "    temperature=0,\n",
    "    openai_api_key = api_key,\n",
    "    openai_api_base = api_base,\n",
    ")\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = \"列出5个{subject}.\\n{format_instructions}\",\n",
    "    input_variables = [\"subject\"],\n",
    "    partial_variables = {\"format_instructions\":parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "_input = prompt.format(subject=\"常见的小狗的名字\")\n",
    "output = model(_input)\n",
    "print(output)\n",
    "\n",
    "parser.parse(output)\n",
    "# ['1. 小白 (Xiao Bai)\\n2. 小黑 (Xiao Hei)\\n3. 小贝 (Xiao Bei)\\n4. 小旺 (Xiao Wang)\\n5. 小强 (Xiao Qiang)']"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
