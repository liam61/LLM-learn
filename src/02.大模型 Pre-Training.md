# 大模型 Pre-Training.md

## LayerNorm vs BatchNorm

Transformer 中使用 LayerNorm 而不是 BatchNorm

Norm => Normalization 归一化

归一化是将数据缩放到0-1之间，标准化是将数据缩放到均值为0，方差为1的正态分布

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E6%90%9E%E6%B8%85%E6%A5%9A%E4%B8%BA%E4%BB%80%E4%B9%88Transformer%E4%B8%AD%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm/0.png)

## BatchNorm

BatchNorm 旨在提高神经网络和训练速度、稳定和性能

BatchNorm 主要解决的问题是在训练深度神经网络时出现的内部协变量偏移（Internal Covariate Shift），即网络中各层输入数据分布的变化

内部协变量偏移是指，随着网络层次的加深，每一层的输入数据（即前一层的输出）的分布可能会发生变化

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E6%90%9E%E6%B8%85%E6%A5%9A%E4%B8%BA%E4%BB%80%E4%B9%88Transformer%E4%B8%AD%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm/1.png)

## LayerNorm

Layer Normalization（层归一化）是一种在深度学习中用于稳定神经网络训练和提高性能的技术

可以减少 BatchNorm 中的噪声问题，并允许网络使用更小的小批量大小进行训练

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E6%90%9E%E6%B8%85%E6%A5%9A%E4%B8%BA%E4%BB%80%E4%B9%88Transformer%E4%B8%AD%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm/2.png)

## Why LayerNorm

我们需要确保参与归一化的数据点在本质上是可比的

LayerNorm的解决方案是对每个样本的所有特征进行单独归一化，而不是基于整个批次。这就像是评估每个学生在所有科目中的表现，而不是仅仅关注单一科目，这样可以更全面地理解每个学生的整体表现

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/10%E5%88%86%E9%92%9F%E6%90%9E%E6%B8%85%E6%A5%9A%E4%B8%BA%E4%BB%80%E4%B9%88Transformer%E4%B8%AD%E4%BD%BF%E7%94%A8LayerNorm%E8%80%8C%E4%B8%8D%E6%98%AFBatchNorm/3.png)

# 混合专家模型 (MoE）

## 混合专家模型 (MoEs):

- 与稠密模型相比， 预训练速度更快
- 与具有相同参数数量的模型相比，具有更快的 推理速度
- 需要 大量显存，因为所有专家系统都需要加载到内存中
- 在微调方面存在诸多挑战

## 什么是 MoE

模型规模是提升模型性能的关键因素之一。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳

![alt text](https://luhengshiwo.github.io/LLMForEverybody/01-%E7%AC%AC%E4%B8%80%E7%AB%A0-%E9%A2%84%E8%AE%AD%E7%BB%83/assest/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B%20%28MoE%29%20%E8%AF%A6%E8%A7%A3%EF%BC%88%E8%8A%82%E9%80%89%EF%BC%89/0.png)

## 什么是稀疏性

条件计算的概念 (即仅在每个样本的基础上激活网络的不同部分) 使得在不增加额外计算负担的情况下扩展模型规模成为可能。这一策略在每个 MoE 层中实现了数以千计甚至更多的专家的有效利用

# Mamba

OpenAI 的 ChatGPT、Google 的 Gemini 和 GitHub 的 Copilot 都是由 Transformer 驱动的.然而，Transformer 有一个根本缺陷：它是由 Attention 驱动，而 Attention 会随着序列长度的平方增长

对于快速交流（要求 ChatGPT 讲个笑话），这没问题.但对于需要大量单词的查询（要求 ChatGPT 总结一份 100 页的文档），Transformer 可能会变得非常慢

## 背景

Mamba 架构主要基于 S4。这是一种最新的状态空间模型 state space model (SSM) 架构

